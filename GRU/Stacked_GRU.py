# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Ks9tdriV_VdAOKFN7AHSlLVmeyXKyTd
"""

import pandas as pd
import numpy as np
import datetime as dt
import math
import yfinance
import matplotlib.pyplot as plt


from sklearn.metrics import mean_squared_error,mean_absolute_error
from sklearn.preprocessing import MinMaxScaler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,LSTM,GRU

from itertools import cycle

fdx = yfinance.Ticker('FDX')
fdx_price = fdx.history(period="10y")

fdx_price.info()
print("Null values:", fdx_price.isnull().values.sum()) # checking for null and na
print("NA:", fdx_price.isna().values.any())

fdx_price['date']=fdx_price.index.to_series()
fdx_close=fdx_price[["date","Close"]]

# Train, test split
train_size = int(len(fdx_close)*0.90)
train_data,test_data = fdx_close[0:train_size],fdx_close[train_size:len(fdx_close)]

del train_data['date']
del test_data['date']

# Scaling to the range of [0,1]
scaler = MinMaxScaler(feature_range=(0,1))
train_data = scaler.fit_transform(np.array(train_data).reshape(-1,1))
test_data = scaler.transform(np.array(test_data).reshape(-1,1))

def sliding_window(dataset, time_step=1):
  datax,datay=[],[]
  for i in range(len(dataset)-time_step-1):
    a = dataset[i:(i+time_step),0]
    datax.append(a)
    datay.append(dataset[i+time_step,0])
  return np.array(datax),np.array(datay)

time_step = 30
x_train,y_train = sliding_window(train_data,time_step)
x_test,y_test = sliding_window(test_data,time_step)

x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)
x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)

modelGRU = Sequential()
modelGRU.add(GRU(units=64, return_sequences=True, input_shape=(time_step,1)))
modelGRU.add(GRU(units=64, return_sequences=True))
modelGRU.add(GRU(units=64))
modelGRU.add(Dropout(0.2))
modelGRU.add(Dense(units=1))

# Compiling
modelGRU.compile(optimizer='adam',loss='mean_squared_error')

modelGRU.summary()

r1 = modelGRU.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=150,batch_size=64,verbose=1)

loss = r1.history['loss']
t_loss = r1.history['val_loss']
epochs = range(len(loss))
plt.plot(epochs,loss,label="Training loss")
plt.plot(epochs,t_loss,label="Testing loss")
plt.legend(loc=0)
plt.title("Training and testing loss")

train_predict = scaler.inverse_transform(modelGRU.predict(x_train))
test_predict = scaler.inverse_transform(modelGRU.predict(x_test))
y_train2 = scaler.inverse_transform(y_train.reshape(-1,1))
y_test2 = scaler.inverse_transform(y_test.reshape(-1,1))

mean_squared_error(y_test2,test_predict)

plt.plot(test_predict,label="prediction")
plt.plot(y_test2,label="test data")
plt.legend(loc=2)
plt.title("Prediction")